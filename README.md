# Neuroverse: Adaptive VR for Neurodivergent Empathy
[![Docs](https://img.shields.io/badge/wiki-Documentation-blue?logo=github)](https://github.com/Ziforge/Neuroverse/wiki)
[![Overleaf](https://img.shields.io/badge/View%20Thesis-Overleaf-brightgreen?logo=Overleaf&logoColor=white)](https://www.overleaf.com/read/nddwcrqrpbcs#c4cd87)
---
[![Weekly Timesheet](https://img.shields.io/badge/Open_This_Week's_Timesheet-blue?style=for-the-badge)](https://github.com/Ziforge/Neuroverse/blob/main/weekly_notes/week-13-timesheet.md)
[![ðŸ“‹ Neuroverse Sprint Board](https://img.shields.io/badge/Project%20Board-Neuroverse-green?style=for-the-badge)](https://github.com/users/Ziforge/projects/1/views/1)
[![ðŸ§  Sensory Calibration Questionnaire](https://img.shields.io/badge/Questionnaire-%F0%9F%A7%A0-blue?style=for-the-badge)](https://github.com/Ziforge/Neuroverse/wiki/Sensory-Calibration-Questionnaire)
[![ðŸ›¡ï¸ GDPR Consent Form](https://img.shields.io/badge/GDPR%20Consent-View%20Policy-blue?style=for-the-badge)](https://github.com/Ziforge/Neuroverse/wiki/GDPR-Consent-Form)
> ðŸŒ A VR platform that adapts to individual sensory needs â€” connecting neurodivergent and neurotypical users through shared, empathetic interaction.

This project explores adaptive sensory filtering and profile blending in shared VR experiences between neurotypical and neurodivergent individuals. Built in Unity using C# and deployed on Meta Quest 3.

## ðŸ§  Features

- Sensory calibration and profile prediction using ML
- Real-time filtering (audio, visual, motion)
- Shared user blending engine
- GDPR-compliant anonymous data collection
- Modular Unity-based architecture

âž¡ï¸ **[See Full Development Guide on GitHub Wiki](https://github.com/Ziforge/Neuroverse/wiki/Development-Setup-Guide)**


## ðŸ” Flow Chart

```mermaid
flowchart TD
    A[Start VR Session] --> B[Consent + GDPR Notice]
    B --> C[Calibration Scene]
    C --> D[Passive Behavior Tracking]
    D --> E[ML Predicts Sensory Profile]
    E --> F{Is Other User Nearby?}
    F -- Yes --> G[Blend Sensory Profiles]
    F -- No --> H[Continue Solo Experience]
    G --> I[Adapt Environment]
    H --> I
    I --> J[Cooperative Task or Puzzle]
    J --> K[Post-Session Debrief]
    K --> L[Session Ends]
```


## ðŸ“‚ Structure

- `docs/`: Academic LaTeX report and diagrams  
- `unity_project/`: Unity C# source files  
- `data/`: Anonymized datasets  
- `website/`: HTML files for recruitment and consent  

## ðŸ§ª Run the Study

1. Clone the repo  
2. Open `unity_project/` in Unity 2022+  
3. Load the calibration or experience scenes  
4. Start testing on Meta Quest 3  

## ðŸ“œ License

Licensed under GPL-3.0 or MIT

## ðŸ‘¥ Contributing

See [Contributing Guide on the Wiki](https://github.com/Ziforge/Neuroverse/wiki/Contributingâ€toâ€Neuroverse)


```mermaid
flowchart TD
    A[ðŸ§  Raise Wrist] --> B{Input Type?}

    B -- Hand Tracking âœ‹ --> C[ðŸ‘† Tap Wrist to Open UI]
    B -- Controller ðŸŽ® --> D[ðŸŽ® Press Menu Button]
    B -- Gesture ðŸ«° --> E[âœ‹ Wrist Pinch Activation]

    C --> F[ðŸ“º Show Sensory Menu Panel]
    D --> F
    E --> F

    F --> G{Choose Tab}
    G --> G1[ðŸ”† Sensory Filters]
    G --> G2[ðŸŽ® Presets]
    G --> G3[ðŸ§‘â€ðŸ¦± Profile]
    G --> G4[ðŸŒ Environment]

    G1 --> H1[Adjust Sliders (Visual, Audio, Haptics, Motion)]
    G2 --> H2[Select Calm / Stim / Focus Mode]
    G3 --> H3[Save/Load Profile]
    G4 --> H4[Adjust Lighting, Space, Background]

    H1 --> I[âœ… Save Settings]
    H2 --> I
    H3 --> I
    H4 --> I

    I --> J[ðŸ§˜ Return to VR World]

    style F fill:#f9f,stroke:#333,stroke-width:2px
    style G1 fill:#ccf,stroke:#000
    style G2 fill:#ccf,stroke:#000
    style G3 fill:#ccf,stroke:#000
    style G4 fill:#ccf,stroke:#000
