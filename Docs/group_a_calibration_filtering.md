## âœ… Key Achievements

### 1. LaTeX Thesis Document & Scientific Foundation
- Created a structured masterâ€™s-level academic document in Overleaf
- Integrated sections on:
  - Neurodivergent conditions (autism, ADHD, schizophrenia, bipolar disorder)
  - Sensory adaptation frameworks and empathy-based interaction
  - Methodology, adaptive filtering techniques, ethical compliance (GDPR)
- Included scientific references and citations throughout

---

### 2. GitHub Wiki Setup & Documentation Infrastructure
- Built a structured GitHub Wiki system:
  - Navigation sidebar, home page, and linked markdown pages
  - Pages for Unity architecture, ML model design, GDPR & consent, perceptual interface design, and group responsibilities
  - Weekly timesheet logging and visual reporting

---

### 3. Prototype Sensory Filtering System (Unity)
- Created an isolated prototype environment (`experimental/SensoryPrototype/`) with:
  - Adaptive C# scripts for visual, audio, motion, and haptic filtering
  - Slider-based UI for real-time adjustment using perceptual language (e.g. â€œDimâ€, â€œStrongâ€, â€œNaturalâ€)
  - Profile saving/loading via JSON
  - Fully documented integration approach in both code and markdown

---

### 4. Project Planning, Automation & Sprint Tracking
- Configured a full GitHub Sprint Board with:
  - Weekly columns (1â€“8), labels per team group, and automation scripts
  - CLI and GraphQL-based issue creation for task tracking and review
  - Synced planning templates and milestone organization with group collaboration

---

### 5. Time Tracking & Visual Analytics
- Created a flexible system for weekly time tracking:
  - Markdown timesheets per week
  - Python-based graph generation for cumulative effort and goal completion
  - Badges and visual summaries embedded in README and Wiki

---

### 6. Machine Learning Planning & Mock Data
- Designed behavioral input feature set for future ML models:
  - Inputs include gaze tracking, posture, head motion, haptic sensitivity, culture, gender, and sensory history
- Drafted mock JSON datasets for simulation and calibration
- Mapped out potential integration using ML.NET, TFLite, and Unity Barracuda

---

### 7. Group A â€“ Calibration & Adaptive Filtering Methodology
- Defined initial workflow for passive user calibration via UI sliders and JSON persistence
- I
âœ… Sprint Log

â¸»

ğŸ› ï¸ Main Technical Contributions

ğŸ” Calibration System Expansion
â€¢ Refactored and finalized CalibrationLogger.cs:
â€¢ Captures real-time perceptual input with timestamps and context
â€¢ Maps values to user-friendly perceptual labels (e.g., â€œMutedâ€, â€œNaturalâ€)
â€¢ Serializes into timestamped JSON files
â€¢ Honors GDPR consent toggle before any logging occurs
â€¢ Developed and implemented CalibrationLoader.cs:
â€¢ Automatically scans for calibration log files
â€¢ Populates Unity UI dropdown for user selection
â€¢ Reloads values into sliders and rehydrates session state

â¸»

ğŸ§ª Research & Theoretical Development

ğŸ“„ LaTeX Academic Report (Overleaf)
â€¢ Added two major new sections:
â€¢ ğŸ§  Embodied Movement Strategies for Neurodivergent Accessibility
â€¢ ğŸ›ï¸ UI Paradigm for Neurodivergent Interaction: Wrist-Based Modular Menus
â€¢ Provided academic backing and references on:
â€¢ Teleportation vs. smooth locomotion
â€¢ Importance of proprioception, comfort zones, and reduced visual strain
â€¢ Menu ergonomics (e.g. Fallout-style Pip-Boy inspiration)
â€¢ Use of perceptual terms across sensory filters for improved inclusivity
â€¢ Integrated key sources:
â€¢ Kane et al. on accessibility in immersive interfaces
â€¢ Schwind et al. on VR locomotion and motion sickness mitigation
â€¢ Lindgren et al. on cognitive load and menu orientation in AR/VR

â¸»

ğŸ” GDPR & Consent System
â€¢ Rewrote full GDPR form (Markdown + LaTeX) to reflect:
â€¢ Optionality of haptics, motion, visual/audio adaptation
â€¢ Clear statement of anonymized data handling
â€¢ Participant right to withdraw at any time
â€¢ Added opt-in toggle into the calibration logger with enforcement


ğŸ› ï¸ Tasks & Implementation

ğŸ”¹ 1. Create a CalibrationLogger.cs Script
â€¢ Hook into Unity UI sliders and buttons
â€¢ Write values and timestamps to a structured JSON

ğŸ”¹ 2. Connect Logging to UI
â€¢ Every time a value changes, capture:
â€¢ The new value
â€¢ The perceptual label selected
â€¢ The context (e.g., â€œUser is adjusting audio sensitivityâ€)

ğŸ”¹ 3. Save the Log
â€¢ Use Application.persistentDataPath to store locally
â€¢ Name files with session timestamps:
e.g. calibration_log_2025-03-29_13-24.json

ğŸ”¹ 4. GDPR: Ask for Consent
â€¢ Add a checkbox before the session:
â€œâœ… I agree to store anonymous sensory calibration data.â€
â€¢ Donâ€™t log if unchecked


âœ… Sprint Log

â¸»

ğŸ› ï¸ Main Technical Contributions

ğŸ” Calibration System Expansion
â€¢ Refactored and finalized CalibrationLogger.cs:
â€¢ Captures real-time perceptual input with timestamps and context
â€¢ Maps values to user-friendly perceptual labels (e.g., â€œMutedâ€, â€œNaturalâ€)
â€¢ Serializes into timestamped JSON files
â€¢ Honors GDPR consent toggle before any logging occurs
â€¢ Developed and implemented CalibrationLoader.cs:
â€¢ Automatically scans for calibration log files
â€¢ Populates Unity UI dropdown for user selection
â€¢ Reloads values into sliders and rehydrates session state

â¸»

ğŸ§ª Research & Theoretical Development

ğŸ“„ LaTeX Academic Report (Overleaf)
â€¢ Added two major new sections:
â€¢ ğŸ§  Embodied Movement Strategies for Neurodivergent Accessibility
â€¢ ğŸ›ï¸ UI Paradigm for Neurodivergent Interaction: Wrist-Based Modular Menus
â€¢ Provided academic backing and references on:
â€¢ Teleportation vs. smooth locomotion
â€¢ Importance of proprioception, comfort zones, and reduced visual strain
â€¢ Menu ergonomics (e.g. Fallout-style Pip-Boy inspiration)
â€¢ Use of perceptual terms across sensory filters for improved inclusivity
â€¢ Integrated key sources:
â€¢ Kane et al. on accessibility in immersive interfaces
â€¢ Schwind et al. on VR locomotion and motion sickness mitigation
â€¢ Lindgren et al. on cognitive load and menu orientation in AR/VR

â¸»

ğŸ” GDPR & Consent System
â€¢ Rewrote full GDPR form (Markdown + LaTeX) to reflect:
â€¢ Optionality of haptics, motion, visual/audio adaptation
â€¢ Clear statement of anonymized data handling
â€¢ Participant right to withdraw at any time
â€¢ Added opt-in toggle into the calibration logger with enforcement


ğŸ”— OSC Communication
â€¢ âœ… Installed and configured extOSC in Unity
â€¢ âœ… Hooked up OSC Receiver in Unity to:
â€¢ Listen on 127.0.0.1:9001
â€¢ Receive /face_emotion messages
â€¢ âœ… Created EmotionHandler.cs to:
â€¢ Parse OSC messages and update emotion string
â€¢ Prepare for gesture inputs and feedback
â€¢ Visualize data via Text + Material Color

âœ‹ Hand/Body Gesture Tracking (Unity-side)
â€¢ âœ… Integrated OpenXR and enabled Meta Quest Features:
â€¢ Hand Tracking (experimental)
â€¢ Gesture detection logic via controller inputs (e.g., CommonUsages)
â€¢ âœ… Implemented gesture state parsing and basic gesture classification: Open, Pinch, Point, etc.
â€¢ âœ… Setup OSC sender (soon to be connected for gestures too)

ğŸ§ª XR Deployment & Testing
â€¢ âœ… Enabled Developer Mode on Meta Quest 3
â€¢ âœ… Installed Android Platform Tools + ADB on macOS
â€¢ âœ… Connected Quest via USB and then via ADB TCP/IP
â€¢ âœ… Built and deployed Unity project to the Quest
â€¢ âœ… Confirmed launch and runtime logs working on device
